# ─────────────────────────────────────────────────────────────────────────────
# sources.yaml  —  Fully agentic configuration
#
# NO hardcoded topics or websites. The LLM drives all discovery.
# Config only sets limits, credentials, and quality filters.
# ─────────────────────────────────────────────────────────────────────────────

settings:
  # ── Crawl settings ─────────────────────────────────────────────────────
  concurrent_tasks: 15           # crawl4ai parallel workers
  max_content_chars: 10000
  min_content_chars: 50          # low — CVE pages can be short but still valid
  max_total_urls: 220            # efficient cap for Round 1 URLs
  crawl_page_timeout_ms: 30000   # fail fast on slow domains (default crawl4ai is 60000)
  crawl_wait_until: domcontentloaded
  crawl_cache_mode: enabled      # reuse prior crawl results across runs
  tavily_inter_query_delay_sec: 0.0
  output_file: data/raw_blogs.json

  # ── Agentic pipeline settings ──────────────────────────────────────────
  n_search_queries: 12           # fewer, higher-yield queries
  max_results_per_query: 8       # lower fan-out per query
  max_harvested_urls: 40         # keep Phase 5b bounded
  n_round2_queries: 10           # Phase 6: LLM gap analysis follow-up queries
  max_round2_urls: 150           # Phase 7: cap on Round 2 discovery

  # ── Tavily — search API built for AI agents ────────────────────────────
  # Key: https://app.tavily.com  (free: 1000 searches/month)
  tavily_api_key_env: TAVILY_API_KEY

  # ── Groq — LLM for query generation + gap analysis ─────────────────────
  # Key: https://console.groq.com/keys
  groq_api_key_env: GROQ_API_KEY
  # Example models:
  #   llama-3.3-70b-versatile
  #   llama-3.1-8b-instant
  llm_model: llama-3.3-70b-versatile
  single_llm_request_mode: true   # exactly one OpenRouter request (planning only)
  enable_llm_chain_extraction: false  # disable per-page LLM calls inside crawl workers

# ── Quality filter — ANY match keeps the page ────────────────────────────────
quality_keywords:
  - cve
  - vulnerability
  - exploit
  - payload
  - injection
  - overflow
  - rce
  - xss
  - sqli
  - ssrf
  - bypass
  - poc
  - deserialization
  - privilege escalation
  - remote code execution
  - authentication bypass
  - exploit chain
  - campaign
  - threat actor
  - cwe

# ── Structured signals to extract from every page ────────────────────────────
# These feed directly into build_correlations.py + build_cooccurrence.py
extraction_targets:
  cve_pattern: 'CVE-\d{4}-\d+'
  cwe_pattern: 'CWE-\d+'
  cvss_pattern: 'CVSS[v23\s:]+[\d.]+'
  owasp_pattern: 'A\d{2}:202[1-9]'
  exploit_chain_phrases:
    - "chained with"
    - "combined with"
    - "exploit chain"
    - "followed by"
    - "leveraged alongside"
    - "initial access"
    - "privilege escalation"
    - "lateral movement"
  campaign_phrases:
    - "ransomware campaign"
    - "threat actor"
    - "APT"
    - "nation-state"
    - "attributed to"
    - "exploited in the wild"
    - "actively exploited"

# ── Always-on dynamic sources (no LLM, pure API) ────────────────────────────
dynamic:
  vulhub:
    enabled: true
    max_readmes: 120             # efficient dynamic expansion cap
    api_url: "https://api.github.com/repos/vulhub/vulhub/git/trees/master?recursive=1"
    raw_base: "https://raw.githubusercontent.com/vulhub/vulhub/master/"

  nvd_references:
    enabled: true
    max_urls: 80
    nvd_data_path: data/raw_nvd.json
    top_cvss_count: 1000         # pull from top 1000 high-CVSS records
    allowed_domains:
      - "blog."
      - "research."
      - "portswigger"
      - "rapid7"
      - "qualys"
      - "tenable"
      - "snyk"
      - "github.com/"
      - "exploit-db"
      - "packetstorm"
      - "security.googleblog"
      - "googleprojectzero"
      - "cisa.gov"
      - "msrc.microsoft.com"
      - "securitylab.github"

# ── No hardcoded topics — the LLM decides what to search for ─────────────────
# Phase 1: LLM generates n_search_queries queries covering vuln correlations,
#           exploit chains, APT campaigns, CWE families, product clusters, etc.
# Phase 6: LLM reviews Round 1 gaps and generates n_round2_queries follow-ups.
