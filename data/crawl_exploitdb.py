"""
crawl_exploitdb.py
------------------
Downloads Exploit-DB's full exploit/shellcode CSV exports.
This is FAR better than scraping individual Exploit-DB pages because:
  - One download gets the entire database (70k+ exploits)
  - Official export â€” structured, clean, no scraping needed
  - Updated daily at https://gitlab.com/exploit-database/exploitdb
  - Includes: exploit code, CVE cross-references, platform, author, date

Output: raw_exploitdb.json

Alternative (if CSV not available): clones the Exploit-DB GitLab repo.
"""

import requests
import json
import csv
import re
import time
import subprocess
import tempfile
import os
from pathlib import Path
from io import StringIO
from tqdm import tqdm

from crawl_cache import is_stale, latest_date_field, record_count

# â”€â”€ Exploit-DB official CSV exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXPLOITDB_FILES_CSV    = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
EXPLOITDB_SHELLCODES_CSV = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_shellcodes.csv"
EXPLOITDB_RAW_BASE     = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/exploits"


def download_exploitdb_csv(url: str, label: str) -> list[dict]:
    """
    Download and parse an Exploit-DB CSV file.
    CSV columns: id, file, description, date_published, author, type,
                 platform, port, date_added, date_updated, verified, codes, tags
    'codes' column contains CVE IDs (semicolon-separated).
    """
    print(f"Downloading Exploit-DB {label} CSV...")
    try:
        resp = requests.get(url, timeout=60)
        resp.raise_for_status()

        reader  = csv.DictReader(StringIO(resp.text))
        records = []

        for row in reader:
            # 'codes' field contains CVE IDs like "CVE-2021-44228;CVE-2021-45046"
            codes_raw = row.get("codes", "")
            cves      = [c.strip() for c in codes_raw.split(";") if c.strip().startswith("CVE-")]

            # Also regex-scan description for CVE mentions
            desc = row.get("description", "")
            cves_in_desc = list(set(re.findall(r"CVE-\d{4}-\d+", desc, re.IGNORECASE)))
            cves = list(set(cves + cves_in_desc))

            records.append({
                "source":          "exploit_db",
                "exploit_id":      row.get("id", ""),
                "file_path":       row.get("file", ""),
                "description":     desc,
                "date_published":  row.get("date_published", ""),
                "author":          row.get("author", ""),
                "exploit_type":    row.get("type", ""),
                "platform":        row.get("platform", ""),
                "verified":        row.get("verified", "0") == "1",
                "tags":            row.get("tags", ""),
                "cves_mentioned":  cves
            })

        print(f"  âœ… Exploit-DB {label}: {len(records)} entries ({sum(1 for r in records if r['cves_mentioned'])} with CVEs)")
        return records

    except Exception as e:
        print(f"  âš ï¸  Exploit-DB {label} download failed: {e}")
        return []


def fetch_exploit_content(exploit_id: str, file_path: str, max_chars: int = 3000) -> str:
    """
    Fetch the actual exploit code/content from GitLab raw.
    Only fetches for verified exploits to save bandwidth.
    """
    if not file_path:
        return ""

    # file_path looks like: exploits/linux/remote/12345.py
    raw_url = f"https://gitlab.com/exploit-database/exploitdb/-/raw/main/{file_path}"
    try:
        resp = requests.get(raw_url, timeout=15)
        if resp.status_code == 200:
            return resp.text[:max_chars]
        return ""
    except Exception:
        return ""


def enrich_verified_exploits(records: list[dict], max_to_fetch: int = 50) -> list[dict]:
    """
    For verified exploits that map to CVEs, fetch the actual exploit content.
    This provides real payload/technique data for training.
    Only processes verified exploits to focus on quality.
    """
    verified_with_cves = [
        r for r in records
        if r.get("verified") and r.get("cves_mentioned") and r.get("file_path")
    ]

    print(f"\nEnriching {min(len(verified_with_cves), max_to_fetch)} verified exploits with code content...")

    enriched_count = 0
    for record in verified_with_cves[:max_to_fetch]:
        content = fetch_exploit_content(record["exploit_id"], record["file_path"])
        if content:
            record["exploit_content"] = content
            record["exploit_enriched"] = True
            enriched_count += 1
        time.sleep(0.3)  # Rate limit

    print(f"  âœ… Enriched {enriched_count} exploits with code content")
    return records


def _merge_exploitdb(existing_path: Path, new_records: list[dict]) -> list[dict]:
    """
    Incremental merge: load existing records, deduplicate by exploit_id,
    and merge new records on top.
    """
    existing: dict[str, dict] = {}
    if existing_path.exists():
        try:
            with open(existing_path, encoding="utf-8") as f:
                for rec in json.load(f):
                    eid = rec.get("exploit_id", "")
                    if eid:
                        existing[eid] = rec
        except Exception:
            pass
    before = len(existing)
    for rec in new_records:
        eid = rec.get("exploit_id", "")
        if eid:
            existing[eid] = rec  # new overwrites stale
    merged = list(existing.values())
    print(f"  Incremental merge: {before:,} existing + {len(new_records):,} fetched â†’ {len(merged):,} unique")
    return merged


def run(out: str = "data/raw_exploitdb.json", enrich: bool = True):
    out_path = Path(out)

    # â”€â”€ Cache staleness check (mirrors crawl_nvd.py pattern) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not is_stale(out_path):
        cached = record_count(out_path)
        print(f"  âœ… Exploit-DB cache is fresh â€” skipping fetch ({cached:,} records on disk)")
        return

    all_records: list[dict] = []

    # Download exploits CSV
    exploit_records = download_exploitdb_csv(EXPLOITDB_FILES_CSV, "exploits")
    all_records.extend(exploit_records)

    # Download shellcodes CSV
    shellcode_records = download_exploitdb_csv(EXPLOITDB_SHELLCODES_CSV, "shellcodes")
    all_records.extend(shellcode_records)

    # Optionally fetch exploit code for top verified records
    if enrich:
        all_records = enrich_verified_exploits(all_records, max_to_fetch=100)

    # Incremental merge with existing cache
    all_records = _merge_exploitdb(out_path, all_records)

    # Stats
    with_cves      = [r for r in all_records if r.get("cves_mentioned")]
    verified_count = sum(1 for r in all_records if r.get("verified"))
    platforms      = list(set(r.get("platform", "") for r in all_records if r.get("platform")))

    print(f"\nðŸ“Š Exploit-DB Summary:")
    print(f"  Total records:            {len(all_records)}")
    print(f"  Records with CVE mapping: {len(with_cves)}")
    print(f"  Verified exploits:        {verified_count}")
    print(f"  Platforms covered:        {len(platforms)}")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(all_records, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Saved {len(all_records)} Exploit-DB records â†’ {out_path}")


if __name__ == "__main__":
    run()