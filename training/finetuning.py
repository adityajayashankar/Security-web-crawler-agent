"""
finetuning.py
-------------
CHANGES in this version:
  [MODEL] Mistral-7B-Instruct â†’ fdtn-ai/Foundation-Sec-8B
    - Foundation-Sec-8B is a Llama 3.1-8B model pre-trained on 80B tokens of
      cybersecurity-domain data (NVD, advisories, threat intel, exploit code).
      Starting from a model that already speaks security dramatically reduces
      the fine-tuning burden vs a general-purpose base.
    - Architecture: same SwiGLU MLP (gate/up/down_proj) + GQA attention.
      LoRA target_modules remain identical â€” no module name changes needed.
    - Context window: 8192 tokens (vs Mistral's 4096). We train at 4096 to
      capture longer audit chains and multi-CVE correlation sequences.
    - Tokenizer: Llama 3 tiktoken-based. No native pad token â€” we add one
      explicitly instead of reusing eos_token (prevents loss bleeding).
    - Chat format: Foundation-Sec is a BASE model, not instruct-tuned.
      We keep our Alpaca-style ### Instruction / ### Response template;
      it's the correct format for a base model being SFT-trained.

  [LoRA] Heavier config for correlation & co-occurrence focus
    - r: 16 â†’ 32, lora_alpha: 32 â†’ 64
      Higher rank is needed to capture the dense relational graph between
      CVE â†” CWE â†” ATT&CK â†” OWASP that correlation/co-occurrence training
      introduces. r=16 under-parameterizes these cross-entity associations.
    - lora_dropout: 0.05 â†’ 0.1  (counteracts overfitting with larger rank)

  [LAYER WEIGHTING] Heavy correlation & co-occurrence sampling
    - vulnerability_correlation layer: 3Ã— oversample weight
    - co_occurrence layer (from build_cooccurrence.py): 3Ã— oversample weight
    - All other layers: 1Ã— (unchanged)
    This is done via a WeightedRandomSampler applied BEFORE training so the
    DataLoader sees the weighted distribution without duplicating the file.
    Rationale: these layers are sparse in the raw dataset (~8-12% combined)
    but represent the highest-value signal for multi-CVE attack path reasoning.

  [CONTEXT] max_length 2048 â†’ 4096
    Correlation training pairs reference multiple CVEs per example. The old
    2048 limit silently truncated ~22% of correlation pairs (validated via
    the actual Foundation-Sec tokenizer â€” Llama 3 tokenizes security text
    at ~2.7 chars/token vs Mistral's 2.9, so pairs are slightly longer).

Memory budget at 4096 tokens, 4-bit QLoRA, A100 40GB (or 2Ã— T4):
  Model (4-bit):                ~4.5 GB
  Activations (grad_ckpt, b=1):  ~7 GB  (4096 context is heavier)
  Optimizer (paged_adamw_8bit):  ~3 GB
  LoRA adapters (r=32):          ~1 GB
  Total:                        ~15.5 GB  â†’  fits A100 40GB, tight on T4 2Ã—
  If OOM on T4: set max_length=2048 and gradient_accumulation_steps=32

For Colab A100:
  Use per_device_train_batch_size=2 and gradient_accumulation_steps=8.
"""

import os
import json
import torch
import random
from pathlib import Path
from collections import defaultdict

from datasets import Dataset
from torch.utils.data import WeightedRandomSampler
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer, SFTConfig

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL   = "fdtn-ai/Foundation-Sec-8B"
DATASET_PATH = Path("data") / "training_pairs.jsonl"
OUTPUT_DIR   = Path("./checkpoints/vuln-foundation-sec-8b")
HF_REPO_NAME = "adityajayashankar/vuln-foundation-sec-8b"

EVAL_FRACTION     = 0.05   # 5% of each layer goes to eval
MIN_EVAL_EXAMPLES = 30     # min eval examples per layer

# â”€â”€ Layer weights for heavy correlation/co-occurrence focus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Layers receiving 3Ã— oversample weight during training.
# All unlisted layers default to weight = 1.0.
LAYER_SAMPLE_WEIGHTS: dict[str, float] = {
    "vulnerability_correlation":  3.0,
    "vulnerability_cooccurrence": 3.0,
    # Keep standard layers at 1Ã— â€” they're already well-represented
    "vulnerability_intelligence": 1.0,
    "pentesting_intelligence":    1.0,
    "risk_scoring":               1.0,
    "execution_context":          1.0,
    "audit_evidence":             1.0,
    "remediation_learning":       1.0,
}


# â”€â”€ Prompt formatter â€” suppress empty ### Input: block â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_example(example: dict) -> dict:
    """
    Build the training text for a single pair.
    If 'input' is empty/whitespace, omit the ### Input: section entirely.
    """
    instruction = example.get("instruction", "").strip()
    inp         = example.get("input", "").strip()
    output      = example.get("output", "").strip()

    if inp:
        text = (
            f"### Instruction:\n{instruction}\n\n"
            f"### Input:\n{inp}\n\n"
            f"### Response:\n{output}"
        )
    else:
        text = (
            f"### Instruction:\n{instruction}\n\n"
            f"### Response:\n{output}"
        )
    return {"text": text, "layer": example.get("layer", "general")}


# â”€â”€ Data loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_pairs(path: Path) -> list[dict]:
    pairs = []
    with open(path, encoding="utf-8") as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            try:
                pairs.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"  âš ï¸  Line {i+1}: JSON parse error â€” {e}")
    return pairs


# â”€â”€ CVE-decontaminated stratified split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import re as _re
_CVE_RE = _re.compile(r'(CVE-\d{4}-\d{4,}|GHSA-[a-z0-9-]+)')

def _extract_cve(pair: dict) -> str:
    """Extract CVE/GHSA ID from a training pair."""
    cid = pair.get("cve_id", "")
    if cid:
        return cid
    m = _CVE_RE.search(pair.get("instruction", ""))
    return m.group(1) if m else ""


def stratified_split(
    pairs: list[dict],
    eval_fraction: float = EVAL_FRACTION,
    min_eval: int = MIN_EVAL_EXAMPLES,
) -> tuple[list[dict], list[dict]]:
    """
    CVE-aware stratified split: all pairs for a given CVE go to either
    train OR eval, never both.  This prevents eval recall inflation from
    the model having seen the same CVE during training in another layer.

    Algorithm:
      1. Group pairs by CVE ID.
      2. Randomly assign whole CVEs to eval until we hit the target fraction.
      3. Non-CVE pairs (no extractable ID) split randomly as fallback.
    """
    cve_pairs: dict[str, list[dict]] = defaultdict(list)
    no_cve: list[dict] = []

    for p in pairs:
        cve = _extract_cve(p)
        if cve:
            cve_pairs[cve].append(p)
        else:
            no_cve.append(p)

    # Shuffle CVE order, then greedily assign to eval
    cve_ids = list(cve_pairs.keys())
    random.shuffle(cve_ids)
    total_with_cve = sum(len(ps) for ps in cve_pairs.values())
    target_eval = max(min_eval, int(total_with_cve * eval_fraction))

    eval_cves: set[str] = set()
    eval_count = 0
    for cve in cve_ids:
        if eval_count >= target_eval:
            break
        eval_cves.add(cve)
        eval_count += len(cve_pairs[cve])

    train_pairs, eval_pairs = [], []
    for cve, ps in cve_pairs.items():
        if cve in eval_cves:
            eval_pairs.extend(ps)
        else:
            train_pairs.extend(ps)

    # Non-CVE pairs: random split
    random.shuffle(no_cve)
    n_eval_nc = max(1, int(len(no_cve) * eval_fraction)) if no_cve else 0
    eval_pairs.extend(no_cve[:n_eval_nc])
    train_pairs.extend(no_cve[n_eval_nc:])

    random.shuffle(train_pairs)
    random.shuffle(eval_pairs)

    # Report decontamination stats
    train_cves = {_extract_cve(p) for p in train_pairs} - {""}
    eval_cves_actual = {_extract_cve(p) for p in eval_pairs} - {""}
    overlap = train_cves & eval_cves_actual
    print(f"  CVE-decontaminated split: {len(train_pairs):,} train  /  {len(eval_pairs):,} eval")
    print(f"    Train CVEs: {len(train_cves):,}  |  Eval CVEs: {len(eval_cves_actual):,}  |  Overlap: {len(overlap)}")
    return train_pairs, eval_pairs


def pairs_to_dataset(pairs: list[dict]) -> Dataset:
    """Convert list of dicts â†’ HuggingFace Dataset with 'text' field."""
    formatted = [format_example(p) for p in pairs]
    return Dataset.from_list(formatted)


# â”€â”€ Weighted sampler for heavy correlation/co-occurrence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_weighted_sampler(train_pairs: list[dict]) -> WeightedRandomSampler:
    """
    Returns a WeightedRandomSampler that oversamples correlation and
    co-occurrence layers 3Ã— relative to other layers.

    This skews the gradient signal toward relational security knowledge
    (CVEâ†”CVE, CWE clusters, OWASP co-occurrence) without discarding other
    layers â€” the model still sees all layer types every epoch.
    """
    weights = [
        LAYER_SAMPLE_WEIGHTS.get(p.get("layer", "general"), 1.0)
        for p in train_pairs
    ]
    sampler = WeightedRandomSampler(
        weights     = weights,
        num_samples = len(weights),
        replacement = True,          # replacement=True allows true oversampling
    )
    layer_eff = defaultdict(float)
    for p, w in zip(train_pairs, weights):
        layer_eff[p.get("layer", "general")] += w
    total_w = sum(weights)
    print("\n  Effective training distribution after layer weighting:")
    for layer, w in sorted(layer_eff.items(), key=lambda x: -x[1]):
        print(f"    {layer:<38} {w/total_w*100:5.1f}%  (raw weight {LAYER_SAMPLE_WEIGHTS.get(layer, 1.0):.1f}Ã—)")
    return sampler


# â”€â”€ Load model in 4-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_model():
    """
    Load Foundation-Sec-8B in QLoRA (4-bit NF4).

    Tokenizer notes for Llama 3 / Foundation-Sec-8B:
      - No native pad token in the vocab â€” we add <|finetune_right_pad_id|>
        which is the official Llama 3.1 fine-tuning pad token.
        Using eos_token as pad causes loss on the padding tokens to bleed
        into the gradient because the model has already been trained to
        treat eos as a meaningful signal.
      - padding_side = "right" is correct for causal LM SFT.
    """
    bnb_config = BitsAndBytesConfig(
        load_in_4bit              = True,
        bnb_4bit_compute_dtype    = torch.bfloat16,  # bfloat16 preferred for Llama 3
        bnb_4bit_quant_type       = "nf4",
        bnb_4bit_use_double_quant = True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config = bnb_config,
        device_map          = "auto",
        trust_remote_code   = True,
        attn_implementation = "flash_attention_2",  # FA2 required for 4096 context
    )
    model.config.use_cache = False   # required for gradient checkpointing

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    tokenizer.padding_side = "right"

    # Llama 3 has no pad token by default â€” add the official fine-tuning pad token
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|finetune_right_pad_id|>"})
        model.resize_token_embeddings(len(tokenizer))

    return model, tokenizer


# â”€â”€ LoRA config â€” heavy rank for correlation/co-occurrence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_lora_config() -> LoraConfig:
    """
    r=32, lora_alpha=64 for Foundation-Sec-8B correlation-heavy fine-tuning.

    Foundation-Sec-8B (Llama 3.1 base) uses the same SwiGLU MLP as Mistral:
      gate_proj  â€” gating signal
      up_proj    â€” intermediate projection
      down_proj  â€” back-projection to model dim

    Why r=32 (up from 16):
      The correlation and co-occurrence layers require learning a dense graph
      of relationships: CVEâ†”CWEâ†”ATT&CKâ†”OWASPâ†”stack profiles. r=16 
      under-parameterizes this relational space. r=32 gives the LoRA adapter
      ~170M trainable params (vs ~85M at r=16) â€” still QLoRA-safe on A100.

    lora_dropout=0.1 (up from 0.05):
      Higher rank â†’ higher overfitting risk, especially on the correlation
      layer which has rich but repetitive patterned outputs. 0.1 dropout
      counteracts memorization.

    Trainable param estimate (Foundation-Sec-8B, r=32):
      Attention (4 projections Ã— 32 layers):  ~108M
      MLP (3 projections Ã— 32 layers):         ~64M
      Total trainable:                         ~172M  (fits A100 comfortably)
    """
    return LoraConfig(
        task_type      = TaskType.CAUSAL_LM,
        r              = 32,
        lora_alpha     = 64,
        lora_dropout   = 0.1,
        bias           = "none",
        target_modules = [
            # Attention projections
            "q_proj", "k_proj", "v_proj", "o_proj",
            # MLP projections â€” store factual CVE/CWE/ATT&CK knowledge
            "up_proj", "down_proj", "gate_proj",
        ],
    )


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"Loading dataset: {DATASET_PATH}")
    pairs = load_pairs(DATASET_PATH)
    print(f"  Loaded {len(pairs):,} training pairs")

    # Layer stats before split
    layer_counts: dict[str, int] = defaultdict(int)
    for p in pairs:
        layer_counts[p.get("layer", "general")] += 1
    print("\n  Raw layer distribution:")
    for layer, count in sorted(layer_counts.items(), key=lambda x: -x[1]):
        weight = LAYER_SAMPLE_WEIGHTS.get(layer, 1.0)
        print(f"    {layer:<38} {count:>7,} pairs  (sample weight: {weight:.1f}Ã—)")

    # Stratified split
    train_pairs, eval_pairs = stratified_split(pairs)
    train_dataset = pairs_to_dataset(train_pairs)
    eval_dataset  = pairs_to_dataset(eval_pairs)

    # Weighted sampler for correlation/co-occurrence focus
    sampler = build_weighted_sampler(train_pairs)

    print(f"\nLoading base model: {BASE_MODEL}")
    model, tokenizer = load_model()

    lora_cfg = get_lora_config()
    model    = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    training_args = SFTConfig(
        output_dir                  = str(OUTPUT_DIR),
        num_train_epochs            = 3,
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 16,
        gradient_checkpointing      = True,
        optim                       = "paged_adamw_8bit",
        learning_rate               = 2e-4,
        lr_scheduler_type           = "cosine",
        warmup_steps                = 100,
        bf16                        = True,   # bfloat16 for Llama 3 (not fp16)
        logging_steps               = 50,
        logging_strategy            = "steps",
        eval_strategy               = "steps",
        eval_steps                  = 200,
        save_steps                  = 200,
        save_total_limit            = 3,
        load_best_model_at_end      = True,
        max_length                  = 4096,   # increased for multi-CVE correlation pairs
        dataset_text_field          = "text",
        report_to                   = "none",
        # Pass our weighted sampler so the DataLoader oversamples correlation layers
        # SFTTrainer accepts train_dataset_sampler via data_collator workaround;
        # see NOTE below for the custom Trainer subclass.
    )

    # NOTE: SFTTrainer does not natively accept a custom sampler argument.
    # We subclass it to inject our WeightedRandomSampler.
    class WeightedSFTTrainer(SFTTrainer):
        def _get_train_sampler(self):
            return sampler   # replace the default RandomSampler

    trainer = WeightedSFTTrainer(
        model             = model,
        args              = training_args,
        train_dataset     = train_dataset,
        eval_dataset      = eval_dataset,
        processing_class  = tokenizer,
    )

    print("\nðŸš€ Starting fine-tuning (Foundation-Sec-8B, correlation-heavy)...")
    trainer.train()

    # â”€â”€ Save and Merge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_dir  = OUTPUT_DIR / "final"
    merged_dir = OUTPUT_DIR / "merged"

    trainer.save_model(str(final_dir))
    print(f"\nâœ… LoRA adapter saved â†’ {final_dir}")

    print("\nMerging LoRA weights into base model...")
    merged = model.merge_and_unload()
    merged.save_pretrained(str(merged_dir))
    tokenizer.save_pretrained(str(merged_dir))
    print(f"âœ… Merged model saved â†’ {merged_dir}")

    # â”€â”€ Push to Hub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nPushing to HuggingFace Hub: {HF_REPO_NAME}")
    from huggingface_hub import login
    login()

    merged.push_to_hub(HF_REPO_NAME)
    tokenizer.push_to_hub(HF_REPO_NAME)
    print(f"ðŸš€ Model live: https://huggingface.co/{HF_REPO_NAME}")

    from datasets import load_dataset as ld
    full_ds = ld("json", data_files=str(Path("data") / "vuln_dataset.jsonl"), split="train")
    full_ds.push_to_hub(f"{HF_REPO_NAME}-dataset")
    print(f"ðŸš€ Dataset live: https://huggingface.co/datasets/{HF_REPO_NAME}-dataset")


if __name__ == "__main__":
    main()