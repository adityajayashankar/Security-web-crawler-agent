"""
finetune.py
-----------
Fine-tunes Mistral-7B on the vulnerability dataset using QLoRA.
Uses training_pairs.jsonl (generated by build_dataset.py) which contains
instruction-response pairs covering all 6 dataset layers.

Hardware: 1x A100 (40GB) or RTX 3090 (24GB) â€” ~3-6 hours
          Google Colab Pro A100 also works fine.

Run:  python training/finetune.py
"""

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer, SFTConfig

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL    = "mistralai/Mistral-7B-Instruct-v0.3"
DATASET_PATH  = "data/training_pairs.jsonl"
OUTPUT_DIR    = "./checkpoints/vuln-mistral-7b"
HF_REPO_NAME  = "your-username/vuln-mistral-7b"   # â† change this

# â”€â”€ Prompt format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Must match exactly what agent/tools use at inference time.
PROMPT_TEMPLATE = (
    "### Instruction:\n{instruction}\n\n"
    "### Input:\n{input}\n\n"
    "### Response:\n{output}"
)

def format_example(example):
    return {"text": PROMPT_TEMPLATE.format(
        instruction = example.get("instruction", ""),
        input       = example.get("input", ""),
        output      = example.get("output", "")
    )}

# â”€â”€ Load model in 4-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_model():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,   # saves additional ~0.4GB
    )
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    model.config.use_cache = False        # required for gradient checkpointing

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.pad_token       = tokenizer.eos_token
    tokenizer.padding_side    = "right"   # avoids warnings during training

    return model, tokenizer

# â”€â”€ LoRA config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_lora_config():
    return LoraConfig(
        task_type    = TaskType.CAUSAL_LM,
        r            = 16,          # rank â€” increase to 32/64 for richer data
        lora_alpha   = 32,
        lora_dropout = 0.05,
        bias         = "none",
        target_modules = [
            "q_proj", "v_proj",     # attention projections (always)
            "k_proj", "o_proj",     # include for better quality
        ]
    )

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"Loading base model: {BASE_MODEL}")
    model, tokenizer = load_model()

    lora_cfg = get_lora_config()
    model    = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()
    # Expected output: ~0.8-1% of total params

    print(f"\nLoading dataset: {DATASET_PATH}")
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
    dataset = dataset.map(format_example)
    dataset = dataset.train_test_split(test_size=0.05, seed=42)

    print(f"  Train: {len(dataset['train'])} examples")
    print(f"  Eval:  {len(dataset['test'])} examples")
    print(f"\nLayer distribution in training set:")
    from collections import Counter
    layers = Counter(dataset["train"]["layer"])
    for layer, count in layers.most_common():
        print(f"  {layer:<35} {count:>5}")

    training_args = SFTConfig(
        output_dir                 = OUTPUT_DIR,
        num_train_epochs           = 3,
        per_device_train_batch_size = 4,
        per_device_eval_batch_size  = 4,
        gradient_accumulation_steps = 4,       # effective batch = 16
        gradient_checkpointing      = True,    # saves GPU memory
        optim                       = "paged_adamw_8bit",
        learning_rate              = 2e-4,
        lr_scheduler_type          = "cosine",
        warmup_ratio               = 0.03,
        fp16                       = True,
        logging_steps              = 50,
        evaluation_strategy        = "steps",
        eval_steps                 = 200,
        save_steps                 = 200,
        save_total_limit           = 3,
        load_best_model_at_end     = True,
        max_seq_length             = 1024,
        dataset_text_field         = "text",
        report_to                  = "none",    # set to "wandb" if you use W&B
    )

    trainer = SFTTrainer(
        model         = model,
        args          = training_args,
        train_dataset = dataset["train"],
        eval_dataset  = dataset["test"],
    )

    print("\nğŸš€ Starting fine-tuning...")
    trainer.train()
    trainer.save_model(OUTPUT_DIR + "/final")
    print(f"\nâœ… Model saved to {OUTPUT_DIR}/final")

    # â”€â”€ Merge LoRA into base model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nMerging LoRA weights into base model...")
    merged = model.merge_and_unload()
    merged.save_pretrained(OUTPUT_DIR + "/merged")
    tokenizer.save_pretrained(OUTPUT_DIR + "/merged")
    print(f"âœ… Merged model saved to {OUTPUT_DIR}/merged")

    # â”€â”€ Push to HuggingFace Hub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nPushing to HuggingFace Hub: {HF_REPO_NAME}")
    from huggingface_hub import login
    login()   # paste token from huggingface.co/settings/tokens

    merged.push_to_hub(HF_REPO_NAME)
    tokenizer.push_to_hub(HF_REPO_NAME)
    print(f"ğŸš€ Model live: https://huggingface.co/{HF_REPO_NAME}")

    # â”€â”€ Push dataset too â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from datasets import load_dataset as ld
    full_ds = ld("json", data_files="data/vuln_dataset.jsonl", split="train")
    full_ds.push_to_hub(HF_REPO_NAME + "-dataset")
    print(f"ğŸš€ Dataset live: https://huggingface.co/datasets/{HF_REPO_NAME}-dataset")

if __name__ == "__main__":
    main()